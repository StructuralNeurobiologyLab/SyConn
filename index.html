---
layout: default
---

<header class="header row" id="Top">
	<div class="col">
		<h1>SyConn</h1>

		<p class="subtitle">Automated synaptic connectivity inference<br> for volume electron microscopy</p>

		<ul class="button-list -horizontal">
			<li><a class="button" href="https://github.com/StructuralNeurobiologyLab/SyConn"><i class="icon ion ion-social-github"></i> GitHub Repository</a></li>
			<li><a class="button" href="https://drive.google.com/file/d/10rcsu2L31LSMwZrH_vfsetck7DSWEhHE/view?usp=sharing"><i class="icon ion ion-cube"></i> Sample data with code snapshot</a></li>
			<li><a class="button" href="https://structuralneurobiologylab.github.io/SyConn/documentation"><i class="icon ion ion-university"></i> Documentation</a></li>
			<li><a class="button" href="http://www.nature.com/nmeth/journal/vaop/ncurrent/full/nmeth.4206.html"><i class="icon ion-ios-paper"></i> Publication</a></li>
		</ul>
	</div>

</header>

<!-- <section class="section">
	<article class="row">
		<div class="col">
			<h2 class="subtitle">About the Paper</h2>

			<p>At first, membrane and intracellular space is predicted as barrier (black) and cell components, such as mitochondria (blue), vesicle clouds (green) and synaptic junctions (red) are inferred using a CNN with a 3D perceptive field of view (image below). Additionally, another 3D CNN predicts symmetric and asymmetric synaptic junction regions which get combined with the predicted synaptic junctions later on.</p>
		</div>

		<div class="col">
			<aside class="_no-padding comparison-slider">
				<div class="cocoen">
					<img src="{{ "images/img_182.png" | prepend: site.baseurl }}" alt="">
					<img src="{{ "images/overlay_test_182.png" | prepend: site.baseurl }}" alt="">
				</div>
			</aside>
		</div>
	</article>
</section>
-->


<section class="section">
	<article class="row">
		<div class="col">
			<p><strong>In short:</strong> Teravoxel volume electron microscopy datasets from neural tissue can now be acquired in weeks, but data analysis requires years of manual labor (see also <a href="#Fig1">Fig. 1</a>). The SyConn framework uses deep convolutional neural networks (CNN) and random forest classifiers (RFC) to infer a richly-annotated synaptic connectivity matrix from manual neurite skeleton reconstructions by automatically identifying mitochondria, synapses and their types, axons, dendrites, spines, myelin, somata and cell types.</p>
		</div>

		<div class="col">
			<figure id="Fig1">
				<img src="{{ "/images/timing_plot.png" | prepend: site.baseurl}}" alt="Timing plot comparison">
				<figcaption>
					<strong>Fig. 1:</strong> Estimated times for the dense manual annotations of all individual steps required for acquiring a wiring matrix from EM datasets. Hatched bars are skeletonization times.<br>

					<strong>Volumes:</strong> j012 (songbird Area X): 0.00108 mm³; zebrafish larval brain: 0.048 mm³ 45; mouse brain, grey matter: 112 mm³.
				</figcaption>
			</figure>
		</div>
	</article>

	<article>
		<p>SyConn automates all steps shown in <a href="#Fig1">Fig. 1</a>, but the neurite reconstruction, where it only requires manual skeleton reconstructions instead of volume segmentations (compare hatched bars and full bars in <a href="#Fig1">Fig. 1</a>). Using several recursive 3D CNNs we first detect cell boundaries and all kinds of ultrastructures such as synaptic junctions (<a href="#Fig2">Fig. 2b</a>). Using a ray cast approach we extract a hull for each skeleton based on the boundary prediction (<a href="#Fig2">Fig. 2a</a>), which is then used to map mitochdondria, synaptic junctions and vesicle clouds to each cell (<a href="#Fig2">Fig. 2c</a>). Leveraging this information SyConn uses RFCs to partition neurons into their subcellular parts (axon, dendrite, soma; <a href="#Fig2">Fig. 2d</a>) and to assign them to one of four broad cell types (<a href="#Fig2">Fig. 2e</a>). Finally, we combine this into a richly annotated synaptic connectivity matrix (<a href="#Fig2">Fig. 2f</a>).  </p>
	</article>

	<article>
		<figure id="Fig2">
			<img src="{{ "/images/website_workflow.png" | prepend: site.baseurl}}" alt="SyConn Workflow">
			<figcaption>
				<strong>Fig. 2:</strong> Workflow of SyConn with the most important steps. Inputs are the raw data and manually traced skeletons. The hull in (a) is extracted with a ray casting approach based on a barrier map from a 3D CNN. Ultrastructures in (b) refer to synaptic junctions (red), vesicle clouds (green) and mitochondria (blue), which were also inferred by a 3D CNN. The cell in (d) was automatically partitioned in axon (red), dendrite (light gray) and soma (dark gray) using an RFC. Cells in (e) are GP (left) MSN (right) and shown with their mitochondria (blue). The connectivity matrix in (f) encodes cumulated synaptic area between each cell pair.
			</figcaption>
		</figure>
	</article>

	<article>
		<p>To test SyConn, we detected ultrastructural objects in three serial block-face electron microscopy (SBEM) data sets (zebra finch Area X, size: 97.9 × 95.6 × 115 μm³; zebrafish spinal cord, size: 81.8 × 89.9 × 210 μm³; mouse striatum, size: 17.9 × 15.0 × 69 μm³) and created a wiring diagram of the zebra finch basal-ganglia (Area X) from 612 skeleton reconstructions traced with KNOSSOS (<a href="https://knossostool.org/">knossostool.org</a>).</p>
	</article>
</section>

<section class="section" id="GettingStartedSection">
	<article>
		<header>
			<h2 class="subtitle">Getting Started</h2>
		</header>

		<p>Python 2.7 is required (we recommend Anaconda). SyConn has been tested on Linux distributions (CentOS and Arch Linux).</p>
	</article>

	<article class="inbetween-article">
		<h3 class="tag">Quickstart</h3>

		{% include code_installation.html %}
	</article>

	<article>
		<p>All requirements should be automatically installed, when following the instructions. Together with the provided sample data (<a href="https://drive.google.com/file/d/10rcsu2L31LSMwZrH_vfsetck7DSWEhHE/view?usp=sharing">SyConnDenseCube.zip</a>) including our trained models one is able to run most parts of SyConn.</p>
	</article>
</section>


<section class="section" id="GroundtruthSection">
	<article>
		<header>
			<h2 class="subtitle">Ground Truth</h2>
		</header>

		<p> The provided sample package contains trained CNNs, but no ground truth. Ground truth can be downloaded from the provided links below. The readme file in the zip container explains how the ground truth was used for training in SyConn.

		<p> Zebra finch:</p>
		<p> Synaptic junctions, mitochondria and vesicle clouds: <a href="https://drive.google.com/file/d/10qlEt_wEk_k9gI_75rSSDTzoXhfcUvrL/view?usp=sharing">gt_syconn_bird_sj_vc_mi.zip</a> (5GB) </p>
	</article>

</section>


<section class="section" id="PublicationSection">
	<article>
		<header>
			<h2 class="subtitle">Publication</h2>
		</header>

		<p>SyConn was published in <a href="http://www.nature.com/nmeth/journal/vaop/ncurrent/full/nmeth.4206.html">Nature Methods</a>. If you use parts of this code base in your academic projects, please cite the corresponding publication.</p>
	</article>

	<article class="inbetween-article">
		<h3 class="tag">BibTex</h3>

		{% include bibtex.html %}
	</article>
</section>



<section class="section" id="AboutSection">
	<article>
		<header>
			<h2 class="subtitle">Contributors</h2>
		</header>

		<div class="about-gallery row">
			<div class="col">
				<div class="user -with-picture">
					<img class="image" src="{{ "/images/team/sven.jpg" | prepend: site.baseurl }}">

					<h4>Sven Dorkenwald</h4>
					<p>SyConn development and evaluation</p>
				</div>

				<div class="user">
					<h4>Marius Killinger and Gregor Urban</h4>
					<p>ELEKTRONN development.</p>
				</div>
			</div>

			<div class="col">
				<div class="user -with-picture">
					<img class="image" src="{{ "/images/team/philipp.jpg" | prepend: site.baseurl }}">
					<div class="description">
						<h4>Philipp Schubert</h4>
						<p>SyConn development and evaluation</p>
					</div>
				</div>

				<div class="user">
					<h4>Fabian Svara and Shawn Mikula</h4>
					<p>Dataset contributions</p>
				</div>
			</div>

			<div class="col">
				<div class="user -with-picture">
					<img class="image" src="{{ "/images/team/joergen.png" | prepend: site.baseurl }}">
					<div class="description">
						<h4>Jörgen Kornfeld</h4>
						<p>SyConn development and evaluation</p>
					</div>
				</div>

				<div class="user">
				</div>
			</div>
		</div>
	</article>
</section>

<section class="section">
	<article>
		<p>SyConn v1 was developed in the Denk department at the MPI for Medical Reserach in Heidelberg. The current version is being developed in the Kornfeld lab at the MPI of Biological Intelligence (in foundation) in Martinsried, Germany.</p>
	</article>
</section>

<section class="section">
	<article>
		<h2 class="subtitle">References</h2>

		<p>SyConn makes use of the ELEKTRONN neural network toolkit (<a href="http://elektronn.org">elektronn.org</a>) and the KNOSSOS Python tools (<a href="https://github.com/knossos-project/knossos_utils">github</a>). Neuroglancer and KNOSSOS (<a href="https://knossostool.org/">knossostool.org</a>) are used for visualization and annotation of 3D EM data sets.</p>
	</article>
</section>
